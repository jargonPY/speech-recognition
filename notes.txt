*-------- venv setup -------*
1. python3 -m venv .venv
2. source .venv/bin/activate
3. pip freeze > requirements.txt
4. pip install -r requirements.txt

*-------- Ideas ------------*
    1. Different preprocessing techniques during training (data augmentation)
    2. Ensembles
    3. JSON file should maintain a list of model test scores so that it can be easily
       to run best or 10 best
        - ModelsMetaData needs a method that will be called from test() to update this variable

*-------- Questions --------*
    1. when saving model:
        - save best or save per epoch (if best overwrite old files)?
        - what other data to keep track of (number of total epochs, train/val history)?
            - this is also returned as part of the history object
    2. how much of this is necessary before beginning proper training?
    3. How to implement for Jupyter notebooks?

*-------- To Do ------------*
    1. log preprocessed data and incorporate data validation 
        - need to make sure preprocessing is performed as expected
    2. develop a global logging system with debug/test and training modes
    3. build visualization tools for audio data
    4. allow for a global variable that indicates debugging, which will automatically ensure
       smaller batch sizes and faster run times
        - global variable or environment variable?
    5. metrics and visualizations for comparing models
    6. look into methods for searching the space of model parameters

*-------- Next Steps -------*
1. function to upload/download weight files to the cloud
  - read permissions for everyone, write permissions with credentials
2. logs to track CPU/GPU usage + monitoring performace during training
  - done throguh tensorflow
  - can programmatically access the logs
3. plotting functionality
4. start training!!!

*------- Issues ----------*
One-hot encoding: zero padding then one-hot encoding makes an extra class and "uses" the zero index
    1. one-hot encode before padding
    2. avoid one-hot encoding by using an embedding layer

Batch predictions: right now predictions happen on a sample by sample basis

Specifying which audio file/data to predict on

Ensuring that the loaded model is assigned to the correct layers of encoder/decoder model

When doing validation during training although a different dataset is used, the true text characters are fed 
into the decoder. Same issue if we use the Model.evaluate() != CustomModel.evaluate().

We want to ignore any output past the EOS token, so these tokens should not 
contribute to the loss (they must be masked out). For example, if the model 
outputs “Je bois du lait <eos> oui,” the loss for the last word should be ignored.